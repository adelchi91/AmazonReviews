{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655a4097-c9b8-4383-b64d-5213b856db84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adelchiasta/venv/venv_abwab/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from datetime import datetime\n",
    "# import multiprocessing as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11d160-f0b8-4bfe-8fd5-9988379c4480",
   "metadata": {},
   "source": [
    "# 1. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c4ec6f-74ff-4060-8230-0f68159ec2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file amazon_reviews_beauty.joblib exists in the folder.\n"
     ]
    }
   ],
   "source": [
    "full_path = \"../data/amazon_reviews_beauty.joblib\"\n",
    "file_name = \"amazon_reviews_beauty.joblib\"\n",
    "if os.path.exists(full_path):\n",
    "    print(f\"The file {file_name} exists in the folder.\")\n",
    "    # Load the joblib file\n",
    "    df = joblib.load('../data/amazon_reviews_beauty.joblib')\n",
    "else:\n",
    "    print(f\"The file {file_name} does not exists in the folder. Importing...\")\n",
    "    # Load the review dataset\n",
    "    review_dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", split=\"full\", trust_remote_code=True)\n",
    "    # Load the metadata dataset\n",
    "    meta_dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_All_Beauty\", split=\"full\", trust_remote_code=True)\n",
    "    # Convert datasets to pandas DataFrames\n",
    "    df_reviews = pd.DataFrame(review_dataset).sample(n=10000, random_state=2024)\n",
    "    df_meta = pd.DataFrame(meta_dataset)\n",
    "    # Merge the datasets on parent_asin\n",
    "    df = pd.merge(df_reviews, df_meta, on='parent_asin', how='left', suffixes=('_review', '_meta'))\n",
    "    # Save the DataFrame as a joblib file\n",
    "    joblib.dump(df, '../data/amazon_reviews_beauty.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0323a2-2bf8-49c5-a77a-a9ce4eb2828f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rating', 'title_review', 'text', 'images_review', 'asin',\n",
       "       'parent_asin', 'user_id', 'timestamp', 'helpful_vote',\n",
       "       'verified_purchase', 'main_category', 'title_meta', 'average_rating',\n",
       "       'rating_number', 'features', 'description', 'price', 'images_meta',\n",
       "       'videos', 'store', 'categories', 'details', 'bought_together',\n",
       "       'subtitle', 'author'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c2618d-ef57-4815-b593-817ca4743b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb58b048-3a1a-46dc-9a80-a272fad72ff4",
   "metadata": {},
   "source": [
    "# 2. Outliers scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b324135-f73f-4af4-bb75-3c26de6b66cf",
   "metadata": {},
   "source": [
    "## 2.1 Firt version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32df5feb-b916-46a0-9c82-05aca947a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data preparation complete.\n",
      "Running outlier detection pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                      | 1/5 [00:02<00:10,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity (first 10): [[0.03610338]\n",
      " [0.02530873]\n",
      " [0.04091656]\n",
      " [0.05355231]\n",
      " [0.02495647]\n",
      " [0.02806567]\n",
      " [0.0405637 ]\n",
      " [0.03375799]\n",
      " [0.04532672]\n",
      " [0.02853394]]\n",
      "Mean similarity: 0.035800519662544855\n",
      "Standard deviation of similarity: 0.011623486776102018\n",
      "Z-scores (first 10): [[ 0.0260563 ]\n",
      " [-0.90263711]\n",
      " [ 0.44014649]\n",
      " [ 1.52723454]\n",
      " [-0.93294272]\n",
      " [-0.66544998]\n",
      " [ 0.40978894]\n",
      " [-0.17572411]\n",
      " [ 0.81956509]\n",
      " [-0.62516367]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier detection complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class TemporalOutlierDetector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Detect temporal outliers based on review counts within specified time windows.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_column : str, default='timestamp'\n",
    "        Name of the column containing timestamp information.\n",
    "    window_size : str, default='D'\n",
    "        Size of the time window for grouping reviews (e.g., 'D' for daily).\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit(X, y=None)\n",
    "        Fit method required by scikit-learn's BaseEstimator. Returns self.\n",
    "    transform(X)\n",
    "        Transform method to calculate temporal outlier flags based on review counts.\n",
    "        Returns a numpy array of temporal outlier flags.\n",
    "\n",
    "    Explanation:\n",
    "    ------------\n",
    "    This class groups the data by time periods defined by `window_size`, counts reviews within each period,\n",
    "    and identifies temporal outliers based on a threshold of review counts.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_column='timestamp', window_size='D'):\n",
    "        self.time_column = time_column\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform method to calculate temporal outlier flags based on review counts.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            Input data with timestamp information.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Numpy array containing temporal outlier flags (1 for outlier, 0 otherwise).\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        X['datetime'] = pd.to_datetime(X[self.time_column], unit='s', errors='coerce')\n",
    "        X['review_count'] = X.groupby(X['datetime'].dt.to_period(self.window_size))['datetime'].transform('count')\n",
    "        X['temporal_outlier'] = (X['review_count'] > X['review_count'].quantile(0.95)).astype(int)\n",
    "        return X[['temporal_outlier']].values  # Return numpy array\n",
    "\n",
    "class BehavioralOutlierDetector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Detect behavioral outliers based on user review patterns and rating deviations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_column : str, default='user_id'\n",
    "        Name of the column containing user identifiers.\n",
    "    time_column : str, default='timestamp'\n",
    "        Name of the column containing timestamp information.\n",
    "    rating_column : str, default='rating'\n",
    "        Name of the column containing rating information.\n",
    "    window_size : str, default='D'\n",
    "        Size of the time window for grouping reviews (e.g., 'D' for daily).\n",
    "    review_threshold : int, default=3\n",
    "        Threshold for identifying high-frequency users based on review counts.\n",
    "    rating_deviation_threshold : float, default=1.5\n",
    "        Threshold for identifying users with rating deviations from overall averages.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit(X, y=None)\n",
    "        Fit method required by scikit-learn's BaseEstimator. Returns self.\n",
    "    transform(X)\n",
    "        Transform method to calculate behavioral outlier flags based on user behavior.\n",
    "        Returns a numpy array of behavioral outlier flags.\n",
    "\n",
    "    Explanation:\n",
    "    ------------\n",
    "    This class identifies behavioral outliers among users based on two criteria: high-frequency reviewing\n",
    "    and rating deviations. It computes user review counts and average ratings, identifies high-frequency\n",
    "    users and users with rating deviations, and flags outliers accordingly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, user_column='user_id', time_column='timestamp', rating_column='rating', window_size='D', review_threshold=3, rating_deviation_threshold=1.5):\n",
    "        self.user_column = user_column\n",
    "        self.time_column = time_column\n",
    "        self.rating_column = rating_column\n",
    "        self.window_size = window_size\n",
    "        self.review_threshold = review_threshold\n",
    "        self.rating_deviation_threshold = rating_deviation_threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform method to calculate behavioral outlier flags based on user behavior.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            Input data with user, timestamp, and rating information.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Numpy array containing behavioral outlier flags (1 for outlier, 0 otherwise).\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        X['datetime'] = pd.to_datetime(X[self.time_column], unit='s', errors='coerce')\n",
    "        \n",
    "        user_review_counts = X.groupby([self.user_column, X['datetime'].dt.to_period(self.window_size)]).size().reset_index(name='review_count')\n",
    "        high_frequency_users = user_review_counts[user_review_counts['review_count'] > self.review_threshold][self.user_column].unique()\n",
    "        \n",
    "        user_avg_ratings = X.groupby(self.user_column)[self.rating_column].mean()\n",
    "        overall_avg_rating = X[self.rating_column].mean()\n",
    "        deviating_users = user_avg_ratings[abs(user_avg_ratings - overall_avg_rating) > self.rating_deviation_threshold].index\n",
    "        \n",
    "        X['high_frequency_outlier'] = X[self.user_column].isin(high_frequency_users).astype(int)\n",
    "        X['rating_deviation_outlier'] = X[self.user_column].isin(deviating_users).astype(int)\n",
    "        \n",
    "        return X[['high_frequency_outlier', 'rating_deviation_outlier']].values  # Return numpy array\n",
    "\n",
    "# def text_outliers(text_data, threshold=0.1):\n",
    "#     vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "#     tfidf_matrix = vectorizer.fit_transform(text_data)\n",
    "#     cosine_sim = cosine_similarity(tfidf_matrix, dense_output=False)\n",
    "#     avg_similarity = np.mean(cosine_sim, axis=1)\n",
    "#     return (avg_similarity < threshold).astype(int).reshape(-1, 1)  # Return 2D numpy array\n",
    "\n",
    "def text_outliers(text_data, z_score_threshold):\n",
    "    \"\"\"\n",
    "    Identify text outliers based on cosine similarity scores.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text_data : list or array-like\n",
    "        List of text documents to analyze.\n",
    "    z_score_threshold : float\n",
    "        Threshold value for identifying outliers based on z-scores of cosine similarities.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Numpy array containing text outlier flags (1 for outlier, 0 otherwise).\n",
    "\n",
    "    Explanation:\n",
    "    ------------\n",
    "    This function calculates cosine similarity scores for text documents represented as TF-IDF vectors,\n",
    "    computes z-scores based on the distribution of these scores, and identifies outliers based on a\n",
    "    dynamically determined threshold (`z_score_threshold`).\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_data)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, dense_output=False)\n",
    "    avg_similarity = np.mean(cosine_sim, axis=1)\n",
    "    \n",
    "    # Print debug information\n",
    "    print(\"Average similarity (first 10):\", avg_similarity[:10])\n",
    "    \n",
    "    mean_sim = np.mean(avg_similarity)\n",
    "    std_sim = np.std(avg_similarity)\n",
    "    z_scores = (avg_similarity - mean_sim) / std_sim\n",
    "    \n",
    "    # Print more debug information\n",
    "    print(\"Mean similarity:\", mean_sim)\n",
    "    print(\"Standard deviation of similarity:\", std_sim)\n",
    "    print(\"Z-scores (first 10):\", z_scores[:10])\n",
    "    \n",
    "    return (z_scores < -z_score_threshold).astype(int).reshape(-1, 1)\n",
    "\n",
    "# def main_code(df):\n",
    "# Prepare the data\n",
    "print(\"Preparing data...\")\n",
    "df['helpful_vote'] = pd.to_numeric(df['helpful_vote'], errors='coerce')\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "df['verified_purchase'] = df['verified_purchase'].astype(int)\n",
    "# Ensure text columns are converted to strings\n",
    "text_columns = ['title_review', 'text', 'main_category', 'title_meta', 'description']\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "df['combined_text'] = df['title_review'] + ' ' + df['text'] + ' ' + df['main_category'] + ' ' + df['title_meta'] + ' ' + df['description']\n",
    "\n",
    "# Handle missing values\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_columns:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "# For non-numeric columns, fill with a placeholder\n",
    "non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns\n",
    "for col in non_numeric_columns:\n",
    "    df[col] = df[col].fillna(\"Unknown\")\n",
    "\n",
    "# Convert timestamp to datetime and handle invalid values\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')\n",
    "\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "# Define the feature pipeline\n",
    "feature_pipeline = ColumnTransformer([\n",
    "    ('num', StandardScaler(), ['rating', 'helpful_vote', 'verified_purchase', 'price', 'average_rating', 'rating_number']),\n",
    "    ('text', FunctionTransformer(text_outliers, kw_args={'z_score_threshold': 35}), 'combined_text'),\n",
    "    ('temporal', TemporalOutlierDetector(), ['timestamp']),\n",
    "    ('behavioral', BehavioralOutlierDetector(), ['user_id', 'timestamp', 'rating'])\n",
    "])\n",
    "\n",
    "# Fit and transform the data using the pipeline\n",
    "print(\"Running outlier detection pipeline...\")\n",
    "with tqdm(total=5, desc=\"Processing\") as pbar:\n",
    "    # Transform features\n",
    "    X_transformed = feature_pipeline.fit_transform(df)\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Ensure X_transformed is a 2D numpy array\n",
    "    X_transformed = np.asarray(X_transformed)\n",
    "    if X_transformed.ndim == 1:\n",
    "        X_transformed = X_transformed.reshape(-1, 1)\n",
    "\n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42, n_jobs=-1)\n",
    "    iso_forest_outliers = iso_forest.fit_predict(X_transformed)\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Local Outlier Factor\n",
    "    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, n_jobs=-1)\n",
    "    lof_outliers = lof.fit_predict(X_transformed)\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Add outlier results to the dataframe\n",
    "    df['isolation_forest_outlier'] = (iso_forest_outliers == -1).astype(int)\n",
    "    df['lof_outlier'] = (lof_outliers == -1).astype(int)\n",
    "    # Convert small float values to 0 or 1\n",
    "    df['text_outlier'] = (X_transformed[:, 0] > 0.5).astype(int)\n",
    "    df['temporal_outlier'] = (X_transformed[:, 1] > 0.5).astype(int)\n",
    "    df['high_frequency_outlier'] = (X_transformed[:, 2] > 0.5).astype(int)\n",
    "    df['rating_deviation_outlier'] = (X_transformed[:, 3] > 0.5).astype(int)\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Calculate overall outlier score\n",
    "    df['outlier_score'] = (df['isolation_forest_outlier'] + \n",
    "                           df['lof_outlier'] + \n",
    "                           df['text_outlier'] + \n",
    "                           df['temporal_outlier'] + \n",
    "                           df['high_frequency_outlier'] + \n",
    "                           df['rating_deviation_outlier'])\n",
    "    # Mark as outlier if at least two methods flagged it as an outlier\n",
    "    df['is_outlier'] = (df['outlier_score'] >= 2).astype(int)\n",
    "    pbar.update(1)\n",
    "\n",
    "print(\"Outlier detection complete.\")\n",
    "\n",
    "# # Identify samples most likely to be outliers\n",
    "# top_outliers = df.nlargest(10, 'outlier_score')\n",
    "# print(\"\\nTop 10 potential outliers:\")\n",
    "# print(top_outliers[['rating', 'helpful_vote', 'verified_purchase', 'price', 'outlier_score', 'title_review', 'main_category', 'high_frequency_outlier', 'rating_deviation_outlier']])\n",
    "\n",
    "# # Calculate metrics\n",
    "# total_samples = len(df)\n",
    "# print(f\"\\nOutlier Detection Results:\")\n",
    "# for outlier_type in ['isolation_forest_outlier', 'lof_outlier', 'text_outlier', 'temporal_outlier', 'high_frequency_outlier', 'rating_deviation_outlier']:\n",
    "#     count = df[outlier_type].sum()\n",
    "#     print(f\"{outlier_type}: {count} outliers ({count/total_samples:.2%})\")\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# main_code(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bdb05f1-7207-4695-a897-3a184ad485a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 potential outliers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title_review</th>\n",
       "      <th>text</th>\n",
       "      <th>images_review</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>...</th>\n",
       "      <th>author</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>isolation_forest_outlier</th>\n",
       "      <th>lof_outlier</th>\n",
       "      <th>text_outlier</th>\n",
       "      <th>temporal_outlier</th>\n",
       "      <th>high_frequency_outlier</th>\n",
       "      <th>rating_deviation_outlier</th>\n",
       "      <th>outlier_score</th>\n",
       "      <th>is_outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4354</th>\n",
       "      <td>5.0</td>\n",
       "      <td>All the perfect star reviews on here are correct</td>\n",
       "      <td>All the perfect star reviews on here are corre...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B01D2IXB20</td>\n",
       "      <td>B01D2IXB20</td>\n",
       "      <td>AED24UIY2S3ACO4U5XAHOVF4HU6Q</td>\n",
       "      <td>NaT</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>All the perfect star reviews on here are corre...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I pray this sproduct NEVER gets discontinued !</td>\n",
       "      <td>I have usded this product around my eyes for o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B01IA95GV0</td>\n",
       "      <td>B01IA95GV0</td>\n",
       "      <td>AFA3ROXMMCIMZVLXNGYA2ZXZDNTQ</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>I pray this sproduct NEVER gets discontinued !...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Looks beautiful</td>\n",
       "      <td>Works perfectly in the salon for color process...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B005EUEK4S</td>\n",
       "      <td>B005EUEK4S</td>\n",
       "      <td>AFOMGW34K26S4NHKWSMYTSBITTQQ</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Looks beautiful Works perfectly in the salon f...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Duradero</td>\n",
       "      <td>Siempre que uso este perfume me alagan y creen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B08739QVNW</td>\n",
       "      <td>B0BTJ6SYKB</td>\n",
       "      <td>AE7UXIOTHPPS54LJV2G6EHMTCBFQ</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Duradero Siempre que uso este perfume me alaga...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Perfect üòç</td>\n",
       "      <td>I lovvvvveeeeee it!!!! You can't go wrong with...</td>\n",
       "      <td>[{'attachment_type': 'IMAGE', 'large_image_url...</td>\n",
       "      <td>B06Y22GS6X</td>\n",
       "      <td>B06Y22GS6X</td>\n",
       "      <td>AEBPCYFU5LWAYDN35KZEBND77SCQ</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Perfect üòç I lovvvvveeeeee it!!!! You can't go ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>5.0</td>\n",
       "      <td>If you have smelly pits you need this!!!</td>\n",
       "      <td>Before anything you won‚Äôt receive expired deod...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B072Y3CGRX</td>\n",
       "      <td>B072Y3CGRX</td>\n",
       "      <td>AHPICTZYV5JFT55CXJ2EOAERLYJQ</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>If you have smelly pits you need this!!! Befor...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>5.0</td>\n",
       "      <td>So Cute!</td>\n",
       "      <td>The headbands are so cute!  I love the colors ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B072K1ZW8L</td>\n",
       "      <td>B09FP8PP2K</td>\n",
       "      <td>AFLR6AKBXXIYLBTERI5KAG3I7TTA</td>\n",
       "      <td>NaT</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>So Cute! The headbands are so cute!  I love th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>5.0</td>\n",
       "      <td>The hint you‚Äôve all been waiting for..</td>\n",
       "      <td>Listen up ya‚Äôll - so many people (me included)...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07V6RQGRR</td>\n",
       "      <td>B07V6RQGRR</td>\n",
       "      <td>AFDRB6KJKCI67P3LTWAIULAZGOJA</td>\n",
       "      <td>NaT</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>The hint you‚Äôve all been waiting for.. Listen ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Fantastic!</td>\n",
       "      <td>I really like this product. There is a learnin...</td>\n",
       "      <td>[{'attachment_type': 'IMAGE', 'large_image_url...</td>\n",
       "      <td>B07ZJKVVLW</td>\n",
       "      <td>B07ZJKVVLW</td>\n",
       "      <td>AFD6UD3I66OFS3ZNXACZKTSZPYVQ</td>\n",
       "      <td>NaT</td>\n",
       "      <td>389</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Fantastic! I really like this product. There i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Amazing Deal</td>\n",
       "      <td>Amazing wig!  I can't believe how nice this is...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B00ITMWBFI</td>\n",
       "      <td>B00ITMWBFI</td>\n",
       "      <td>AFVWTIFV725AOUFM32QM74EPQIJA</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Amazing Deal Amazing wig!  I can't believe how...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                      title_review  \\\n",
       "4354     5.0  All the perfect star reviews on here are correct   \n",
       "77       5.0    I pray this sproduct NEVER gets discontinued !   \n",
       "312      5.0                                   Looks beautiful   \n",
       "508      5.0                                          Duradero   \n",
       "1049     5.0                                         Perfect üòç   \n",
       "1481     5.0          If you have smelly pits you need this!!!   \n",
       "1728     5.0                                          So Cute!   \n",
       "3174     5.0            The hint you‚Äôve all been waiting for..   \n",
       "3243     5.0                                        Fantastic!   \n",
       "3297     5.0                                      Amazing Deal   \n",
       "\n",
       "                                                   text  \\\n",
       "4354  All the perfect star reviews on here are corre...   \n",
       "77    I have usded this product around my eyes for o...   \n",
       "312   Works perfectly in the salon for color process...   \n",
       "508   Siempre que uso este perfume me alagan y creen...   \n",
       "1049  I lovvvvveeeeee it!!!! You can't go wrong with...   \n",
       "1481  Before anything you won‚Äôt receive expired deod...   \n",
       "1728  The headbands are so cute!  I love the colors ...   \n",
       "3174  Listen up ya‚Äôll - so many people (me included)...   \n",
       "3243  I really like this product. There is a learnin...   \n",
       "3297  Amazing wig!  I can't believe how nice this is...   \n",
       "\n",
       "                                          images_review        asin  \\\n",
       "4354                                                 []  B01D2IXB20   \n",
       "77                                                   []  B01IA95GV0   \n",
       "312                                                  []  B005EUEK4S   \n",
       "508                                                  []  B08739QVNW   \n",
       "1049  [{'attachment_type': 'IMAGE', 'large_image_url...  B06Y22GS6X   \n",
       "1481                                                 []  B072Y3CGRX   \n",
       "1728                                                 []  B072K1ZW8L   \n",
       "3174                                                 []  B07V6RQGRR   \n",
       "3243  [{'attachment_type': 'IMAGE', 'large_image_url...  B07ZJKVVLW   \n",
       "3297                                                 []  B00ITMWBFI   \n",
       "\n",
       "     parent_asin                       user_id timestamp  helpful_vote  \\\n",
       "4354  B01D2IXB20  AED24UIY2S3ACO4U5XAHOVF4HU6Q       NaT            15   \n",
       "77    B01IA95GV0  AFA3ROXMMCIMZVLXNGYA2ZXZDNTQ       NaT             5   \n",
       "312   B005EUEK4S  AFOMGW34K26S4NHKWSMYTSBITTQQ       NaT             2   \n",
       "508   B0BTJ6SYKB  AE7UXIOTHPPS54LJV2G6EHMTCBFQ       NaT             4   \n",
       "1049  B06Y22GS6X  AEBPCYFU5LWAYDN35KZEBND77SCQ       NaT             4   \n",
       "1481  B072Y3CGRX  AHPICTZYV5JFT55CXJ2EOAERLYJQ       NaT             0   \n",
       "1728  B09FP8PP2K  AFLR6AKBXXIYLBTERI5KAG3I7TTA       NaT            11   \n",
       "3174  B07V6RQGRR  AFDRB6KJKCI67P3LTWAIULAZGOJA       NaT            21   \n",
       "3243  B07ZJKVVLW  AFD6UD3I66OFS3ZNXACZKTSZPYVQ       NaT           389   \n",
       "3297  B00ITMWBFI  AFVWTIFV725AOUFM32QM74EPQIJA       NaT            10   \n",
       "\n",
       "      verified_purchase  ...   author  \\\n",
       "4354                  1  ...  Unknown   \n",
       "77                    0  ...  Unknown   \n",
       "312                   1  ...  Unknown   \n",
       "508                   1  ...  Unknown   \n",
       "1049                  0  ...  Unknown   \n",
       "1481                  0  ...  Unknown   \n",
       "1728                  1  ...  Unknown   \n",
       "3174                  0  ...  Unknown   \n",
       "3243                  1  ...  Unknown   \n",
       "3297                  0  ...  Unknown   \n",
       "\n",
       "                                          combined_text  \\\n",
       "4354  All the perfect star reviews on here are corre...   \n",
       "77    I pray this sproduct NEVER gets discontinued !...   \n",
       "312   Looks beautiful Works perfectly in the salon f...   \n",
       "508   Duradero Siempre que uso este perfume me alaga...   \n",
       "1049  Perfect üòç I lovvvvveeeeee it!!!! You can't go ...   \n",
       "1481  If you have smelly pits you need this!!! Befor...   \n",
       "1728  So Cute! The headbands are so cute!  I love th...   \n",
       "3174  The hint you‚Äôve all been waiting for.. Listen ...   \n",
       "3243  Fantastic! I really like this product. There i...   \n",
       "3297  Amazing Deal Amazing wig!  I can't believe how...   \n",
       "\n",
       "      isolation_forest_outlier  lof_outlier text_outlier temporal_outlier  \\\n",
       "4354                         1            1            1                1   \n",
       "77                           1            1            1                1   \n",
       "312                          1            1            1                0   \n",
       "508                          0            1            1                1   \n",
       "1049                         1            1            1                1   \n",
       "1481                         1            1            1                0   \n",
       "1728                         1            1            1                1   \n",
       "3174                         1            1            1                1   \n",
       "3243                         1            1            1                1   \n",
       "3297                         1            1            1                1   \n",
       "\n",
       "      high_frequency_outlier rating_deviation_outlier outlier_score is_outlier  \n",
       "4354                       0                        1             5          1  \n",
       "77                         0                        0             4          1  \n",
       "312                        0                        1             4          1  \n",
       "508                        0                        1             4          1  \n",
       "1049                       0                        0             4          1  \n",
       "1481                       0                        1             4          1  \n",
       "1728                       0                        0             4          1  \n",
       "3174                       0                        0             4          1  \n",
       "3243                       0                        0             4          1  \n",
       "3297                       0                        0             4          1  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify samples most likely to be outliers\n",
    "top_outliers = df.nlargest(10, 'outlier_score')\n",
    "print(\"\\nTop 10 potential outliers:\")\n",
    "top_outliers#[['rating', 'helpful_vote', 'verified_purchase', 'price', 'outlier_score', 'title_review', 'main_category', 'high_frequency_outlier', 'rating_deviation_outlier']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5159742-2a03-411e-8a53-933db75261bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlier Detection Results:\n",
      "isolation_forest_outlier: 1000 outliers (10.00%)\n",
      "lof_outlier: 1000 outliers (10.00%)\n",
      "text_outlier: 6080 outliers (60.80%)\n",
      "temporal_outlier: 550 outliers (5.50%)\n",
      "high_frequency_outlier: 0 outliers (0.00%)\n",
      "rating_deviation_outlier: 540 outliers (5.40%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "total_samples = len(df)\n",
    "print(f\"\\nOutlier Detection Results:\")\n",
    "for outlier_type in ['isolation_forest_outlier', 'lof_outlier', 'text_outlier', 'temporal_outlier', 'high_frequency_outlier', 'rating_deviation_outlier']:\n",
    "    count = df[outlier_type].sum()\n",
    "    print(f\"{outlier_type}: {count} outliers ({count/total_samples:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbd37fb8-6feb-4444-9382-eaf94b3104d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isolation_forest_outlier</th>\n",
       "      <th>lof_outlier</th>\n",
       "      <th>text_outlier</th>\n",
       "      <th>temporal_outlier</th>\n",
       "      <th>high_frequency_outlier</th>\n",
       "      <th>rating_deviation_outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      isolation_forest_outlier  lof_outlier  text_outlier  temporal_outlier  \\\n",
       "0                            0            0             1                 0   \n",
       "1                            0            0             0                 0   \n",
       "2                            0            0             1                 0   \n",
       "3                            0            0             1                 0   \n",
       "4                            0            0             0                 0   \n",
       "...                        ...          ...           ...               ...   \n",
       "9995                         0            1             1                 0   \n",
       "9996                         0            0             0                 0   \n",
       "9997                         0            0             1                 0   \n",
       "9998                         0            1             1                 0   \n",
       "9999                         0            0             0                 0   \n",
       "\n",
       "      high_frequency_outlier  rating_deviation_outlier  \n",
       "0                          0                         0  \n",
       "1                          0                         0  \n",
       "2                          0                         0  \n",
       "3                          0                         0  \n",
       "4                          0                         0  \n",
       "...                      ...                       ...  \n",
       "9995                       0                         0  \n",
       "9996                       0                         0  \n",
       "9997                       0                         0  \n",
       "9998                       0                         0  \n",
       "9999                       0                         0  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['isolation_forest_outlier', 'lof_outlier', 'text_outlier', 'temporal_outlier', 'high_frequency_outlier', 'rating_deviation_outlier']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3bb7365a-30d3-4463-ad5f-01f22fe02112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.3542795232936078)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df[\"parent_asin\"].value_counts().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7dcf8243-a331-4862-9015-8d16cb05706b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parent_asin\n",
       "1477044280     1\n",
       "6041134546     1\n",
       "B000050FDE     3\n",
       "B000068PBJ     1\n",
       "B000068PBL     1\n",
       "              ..\n",
       "B0C7WQK2QW     1\n",
       "B0C9CWKY9G    17\n",
       "B0CC929DZZ     1\n",
       "B0CDH5TH82     2\n",
       "B0CDNZ7F2V     4\n",
       "Length: 7384, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('parent_asin').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e134c-abe2-4ac9-99dc-316f0c2aea42",
   "metadata": {},
   "source": [
    "## 2.2 Second version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dcb3834-f542-417f-a539-2bc6aab4bc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "class TemporalOutlierDetector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, time_column='timestamp', window_size='D', group_column='parent_asin'):\n",
    "        self.time_column = time_column\n",
    "        self.window_size = window_size\n",
    "        self.group_column = group_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['datetime'] = pd.to_datetime(X[self.time_column], unit='s', errors='coerce')\n",
    "        X['review_count'] = X.groupby([self.group_column, X['datetime'].dt.to_period(self.window_size)])['datetime'].transform('count')\n",
    "        X['temporal_outlier'] = (X['review_count'] > X.groupby(self.group_column)['review_count'].transform(lambda x: x.quantile(0.95))).astype(int)\n",
    "        return X[['temporal_outlier']].values\n",
    "\n",
    "\n",
    "class BehavioralOutlierDetector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, user_column='user_id', time_column='timestamp', rating_column='rating', window_size='D', review_threshold=3, rating_deviation_threshold=1.5, group_column='parent_asin'):\n",
    "        self.user_column = user_column\n",
    "        self.time_column = time_column\n",
    "        self.rating_column = rating_column\n",
    "        self.window_size = window_size\n",
    "        self.review_threshold = review_threshold\n",
    "        self.rating_deviation_threshold = rating_deviation_threshold\n",
    "        self.group_column = group_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['datetime'] = pd.to_datetime(X[self.time_column], unit='s', errors='coerce')\n",
    "\n",
    "        # High-frequency user detection\n",
    "        user_review_counts = X.groupby([self.group_column, self.user_column, X['datetime'].dt.to_period(self.window_size)]).size().reset_index(name='review_count')\n",
    "        high_frequency_users = user_review_counts[user_review_counts['review_count'] > self.review_threshold][self.user_column].unique()\n",
    "\n",
    "        # Rating deviation detection\n",
    "        user_avg_ratings = X.groupby([self.group_column, self.user_column])[self.rating_column].mean().reset_index()\n",
    "        overall_avg_ratings = X.groupby(self.group_column)[self.rating_column].mean().reset_index()\n",
    "        deviating_users = user_avg_ratings.merge(overall_avg_ratings, on=self.group_column, suffixes=('_user', '_overall'))\n",
    "        deviating_users['rating_deviation'] = abs(deviating_users[f'{self.rating_column}_user'] - deviating_users[f'{self.rating_column}_overall'])\n",
    "        deviating_users = deviating_users[deviating_users['rating_deviation'] > self.rating_deviation_threshold][self.user_column].unique()\n",
    "\n",
    "        X['high_frequency_outlier'] = X[self.user_column].isin(high_frequency_users).astype(int)\n",
    "        X['rating_deviation_outlier'] = X[self.user_column].isin(deviating_users).astype(int)\n",
    "\n",
    "        return X[['high_frequency_outlier', 'rating_deviation_outlier']].values\n",
    "\n",
    "# def text_outliers(text_data, description_data, group_data, z_score_threshold):\n",
    "#     \"\"\"\n",
    "#     Identify text outliers based on cosine similarity scores with product descriptions.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     text_data : list or array-like\n",
    "#         List of text documents to analyze (combined text for each review).\n",
    "#     description_data : list or array-like\n",
    "#         List of product descriptions corresponding to each text document.\n",
    "#     group_data : list or array-like\n",
    "#         List of group identifiers (e.g., parent_asin) indicating which product each review belongs to.\n",
    "#     z_score_threshold : float\n",
    "#         Threshold value for identifying outliers based on z-scores of cosine similarities.\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     numpy.ndarray\n",
    "#         Numpy array containing text outlier flags (1 for outlier, 0 otherwise).\n",
    "\n",
    "#     Explanation:\n",
    "#     ------------\n",
    "#     This function calculates cosine similarity scores between each review's combined text and the corresponding\n",
    "#     product description within each product group. It computes z-scores based on the distribution of these scores \n",
    "#     and identifies outliers based on a dynamically determined threshold (`z_score_threshold`).\n",
    "#     \"\"\"\n",
    "#     # Vectorize the text data and description data\n",
    "#     vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    \n",
    "#     combined_text_tfidf = vectorizer.fit_transform(text_data)\n",
    "#     description_tfidf = vectorizer.transform(description_data)\n",
    "    \n",
    "#     # Initialize array to store outlier flags\n",
    "#     outlier_flags = np.zeros(len(text_data))\n",
    "    \n",
    "#     # Process each group separately\n",
    "#     for group_id in np.unique(group_data):\n",
    "#         # Get indices for the current group\n",
    "#         group_indices = np.where(group_data == group_id)[0]\n",
    "        \n",
    "#         # Extract the text data and description data for the current group\n",
    "#         group_texts = combined_text_tfidf[group_indices]\n",
    "#         group_description = description_tfidf[group_indices[0]]  # Assume description is the same for all in group\n",
    "        \n",
    "#         # Compute cosine similarity between each review's combined text and the group's description\n",
    "#         cosine_sim = np.array([cosine_similarity(group_texts[i], group_description).flatten()[0]\n",
    "#                                for i in range(len(group_texts))])\n",
    "        \n",
    "#         # Calculate z-scores for the cosine similarities\n",
    "#         mean_sim = np.mean(cosine_sim)\n",
    "#         std_sim = np.std(cosine_sim)\n",
    "#         z_scores = (cosine_sim - mean_sim) / std_sim\n",
    "        \n",
    "#         # Identify outliers based on z-scores\n",
    "#         group_outliers = (z_scores < -z_score_threshold).astype(int)\n",
    "        \n",
    "#         # Update the outlier flags for the current group\n",
    "#         outlier_flags[group_indices] = group_outliers\n",
    "    \n",
    "#     return outlier_flags.reshape(-1, 1)\n",
    "\n",
    "def text_outliers(text_data, description_data, group_data, z_score_threshold):\n",
    "    group_texts = text_data\n",
    "    description_tfidf = description_data\n",
    "\n",
    "    group_indices = np.where(group_data == group_data[0])[0]\n",
    "    group_description = description_tfidf[group_indices[0]]  # Assuming description is the same for all in group\n",
    "\n",
    "    # Compute cosine similarity between each review's combined text and the group's description\n",
    "    cosine_sim = np.array([cosine_similarity(group_texts[i], group_description).flatten()[0]\n",
    "                           for i in range(group_texts.shape[0])])\n",
    "\n",
    "    # Calculate z-scores for the cosine similarities\n",
    "    mean_sim = np.mean(cosine_sim)\n",
    "    std_sim = np.std(cosine_sim)\n",
    "    z_scores = (cosine_sim - mean_sim) / std_sim\n",
    "\n",
    "    # Identify outliers based on z-score threshold\n",
    "    outliers = np.where(z_scores > z_score_threshold)[0]\n",
    "\n",
    "    return outliers\n",
    "\n",
    "\n",
    "class TextOutlierTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, z_score_threshold=1.5):\n",
    "        self.z_score_threshold = z_score_threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Assuming X is a DataFrame and the columns are named as expected\n",
    "        combined_text = X['combined_text'].values\n",
    "        description = X['description'].values\n",
    "        parent_asin = X['parent_asin'].values\n",
    "        \n",
    "        return text_outliers(combined_text, description, parent_asin, self.z_score_threshold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "df['helpful_vote'] = pd.to_numeric(df['helpful_vote'], errors='coerce')\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "df['verified_purchase'] = df['verified_purchase'].astype(int)\n",
    "# Ensure text columns are converted to strings\n",
    "text_columns = ['title_review', 'text', 'main_category', 'title_meta', 'description']\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "df['combined_text'] = df['title_review'] + ' ' + df['text'] + ' ' + df['main_category'] + ' ' + df['title_meta'] + ' ' + df['description']\n",
    "\n",
    "# Handle missing values\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_columns:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "# For non-numeric columns, fill with a placeholder\n",
    "non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns\n",
    "for col in non_numeric_columns:\n",
    "    df[col] = df[col].fillna(\"Unknown\")\n",
    "\n",
    "# Convert timestamp to datetime and handle invalid values\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')\n",
    "\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "\n",
    "feature_pipeline = ColumnTransformer([\n",
    "    ('num', StandardScaler(), ['rating', 'helpful_vote', 'verified_purchase', 'price', 'average_rating', 'rating_number']),\n",
    "    # ('text', TextOutlierTransformer(z_score_threshold=1.5), ['combined_text', 'description', 'parent_asin']),\n",
    "    ('temporal', TemporalOutlierDetector(group_column='parent_asin'), ['timestamp', 'parent_asin']),\n",
    "    ('behavioral', BehavioralOutlierDetector(group_column='parent_asin'), ['user_id', 'timestamp', 'rating', 'parent_asin'])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a76d5732-6df4-4844-84e8-6be01825dec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running outlier detection pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier detection complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Running outlier detection pipeline...\")\n",
    "with tqdm(total=5, desc=\"Processing\") as pbar:\n",
    "    X_transformed = feature_pipeline.fit_transform(df)\n",
    "    pbar.update(1)\n",
    "\n",
    "    X_transformed = np.asarray(X_transformed)\n",
    "    if X_transformed.ndim == 1:\n",
    "        X_transformed = X_transformed.reshape(-1, 1)\n",
    "\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42, n_jobs=-1)\n",
    "    iso_forest_outliers = iso_forest.fit_predict(X_transformed)\n",
    "    pbar.update(1)\n",
    "\n",
    "    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, n_jobs=-1)\n",
    "    lof_outliers = lof.fit_predict(X_transformed)\n",
    "    pbar.update(1)\n",
    "\n",
    "    df['isolation_forest_outlier'] = (iso_forest_outliers == -1).astype(int)\n",
    "    df['lof_outlier'] = (lof_outliers == -1).astype(int)\n",
    "    df['text_outlier'] = (X_transformed[:, 0] > 0.5).astype(int)\n",
    "    df['temporal_outlier'] = (X_transformed[:, 1] > 0.5).astype(int)\n",
    "    df['high_frequency_outlier'] = (X_transformed[:, 2] > 0.5).astype(int)\n",
    "    df['rating_deviation_outlier'] = (X_transformed[:, 3] > 0.5).astype(int)\n",
    "    pbar.update(1)\n",
    "\n",
    "    df['outlier_score'] = (df['isolation_forest_outlier'] + \n",
    "                           df['lof_outlier'] + \n",
    "                           df['text_outlier'] + \n",
    "                           df['temporal_outlier'] + \n",
    "                           df['high_frequency_outlier'] + \n",
    "                           df['rating_deviation_outlier'])\n",
    "    df['is_outlier'] = (df['outlier_score'] >= 2).astype(int)\n",
    "    pbar.update(1)\n",
    "\n",
    "print(\"Outlier detection complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d73a5f4-c115-4788-9b94-15f817f1d409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlier Detection Results:\n",
      "isolation_forest_outlier: 1000 outliers (10.00%)\n",
      "lof_outlier: 1000 outliers (10.00%)\n",
      "text_outlier: 6080 outliers (60.80%)\n",
      "temporal_outlier: 550 outliers (5.50%)\n",
      "high_frequency_outlier: 0 outliers (0.00%)\n",
      "rating_deviation_outlier: 540 outliers (5.40%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "total_samples = len(df)\n",
    "print(f\"\\nOutlier Detection Results:\")\n",
    "for outlier_type in ['isolation_forest_outlier', 'lof_outlier', 'text_outlier', 'temporal_outlier', 'high_frequency_outlier', 'rating_deviation_outlier']:\n",
    "    count = df[outlier_type].sum()\n",
    "    print(f\"{outlier_type}: {count} outliers ({count/total_samples:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd91d4d-ae9c-4856-85a3-4529ad9e7b0a",
   "metadata": {},
   "source": [
    "## 2.3. Third version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e915b3f9-80a7-4f84-bec0-9dce13618c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalOutlierDetector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, time_column='timestamp', window_size='D', group_column='parent_asin'):\n",
    "        self.time_column = time_column\n",
    "        self.window_size = window_size\n",
    "        self.group_column = group_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['datetime'] = pd.to_datetime(X[self.time_column], unit='s', errors='coerce')\n",
    "        X['review_count'] = X.groupby([self.group_column, X['datetime'].dt.to_period(self.window_size)])['datetime'].transform('count')\n",
    "        X['temporal_outlier'] = (X['review_count'] > X.groupby(self.group_column)['review_count'].transform(lambda x: x.quantile(0.95))).astype(int)\n",
    "        return X[['temporal_outlier']].values\n",
    "\n",
    "\n",
    "class BehavioralOutlierDetector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, user_column='user_id', time_column='timestamp', rating_column='rating', window_size='D', review_threshold=3, rating_deviation_threshold=1.5, group_column='parent_asin'):\n",
    "        self.user_column = user_column\n",
    "        self.time_column = time_column\n",
    "        self.rating_column = rating_column\n",
    "        self.window_size = window_size\n",
    "        self.review_threshold = review_threshold\n",
    "        self.rating_deviation_threshold = rating_deviation_threshold\n",
    "        self.group_column = group_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['datetime'] = pd.to_datetime(X[self.time_column], errors='coerce')\n",
    "\n",
    "        # High-frequency user detection\n",
    "        user_review_counts = X.groupby([self.group_column, self.user_column, X['datetime'].dt.to_period(self.window_size)]).size().reset_index(name='review_count')\n",
    "        high_frequency_users = user_review_counts[user_review_counts['review_count'] > self.review_threshold][self.user_column].unique()\n",
    "\n",
    "        # Rating deviation detection\n",
    "        user_avg_ratings = X.groupby([self.group_column, self.user_column])[self.rating_column].mean().reset_index()\n",
    "        overall_avg_ratings = X.groupby(self.group_column)[self.rating_column].mean().reset_index()\n",
    "        deviating_users = user_avg_ratings.merge(overall_avg_ratings, on=self.group_column, suffixes=('_user', '_overall'))\n",
    "        deviating_users['rating_deviation'] = abs(deviating_users[f'{self.rating_column}_user'] - deviating_users[f'{self.rating_column}_overall'])\n",
    "        deviating_users = deviating_users[deviating_users['rating_deviation'] > self.rating_deviation_threshold][self.user_column].unique()\n",
    "\n",
    "        # Combine both outlier types into a single score\n",
    "        X['behavioral_outlier'] = ((X[self.user_column].isin(high_frequency_users) | \n",
    "                                    X[self.user_column].isin(deviating_users))).astype(int)\n",
    "\n",
    "        return X[['behavioral_outlier']].values\n",
    "\n",
    "class GroupwiseIsolationForest(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, contamination=0.1, random_state=42, max_samples=1000, n_estimators=100):\n",
    "        self.contamination = contamination\n",
    "        self.random_state = random_state\n",
    "        self.max_samples = max_samples\n",
    "        self.n_estimators = n_estimators\n",
    "        self.group_models = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        groups = X[X.columns[-1]]\n",
    "        features = X.iloc[:, :-1]\n",
    "        \n",
    "        for group in groups.unique():\n",
    "            group_mask = (groups == group)\n",
    "            group_features = features[group_mask]\n",
    "            \n",
    "            # If the group has more samples than max_samples, take a random sample\n",
    "            if len(group_features) > self.max_samples:\n",
    "                group_features = group_features.sample(n=self.max_samples, random_state=self.random_state)\n",
    "            \n",
    "            iso_forest = IsolationForest(contamination=self.contamination, \n",
    "                                         random_state=self.random_state, \n",
    "                                         max_samples=min(len(group_features), 256),\n",
    "                                         n_estimators=self.n_estimators, n_jobs=-1)\n",
    "            iso_forest.fit(group_features)\n",
    "            self.group_models[group] = iso_forest\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        groups = X[X.columns[-1]]\n",
    "        features = X.iloc[:, :-1]\n",
    "        outlier_scores = np.zeros(X.shape[0])\n",
    "        \n",
    "        for group in groups.unique():\n",
    "            group_mask = (groups == group)\n",
    "            group_features = features[group_mask]\n",
    "            \n",
    "            if group in self.group_models:\n",
    "                outlier_scores[group_mask] = self.group_models[group].decision_function(group_features)\n",
    "            else:\n",
    "                # If we encounter a new group during transform, we'll use the global mean\n",
    "                outlier_scores[group_mask] = np.mean(list(self.group_models.values())[0].decision_function(group_features))\n",
    "        \n",
    "        return outlier_scores.reshape(-1, 1)\n",
    "        \n",
    "class GroupwiseLOF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_neighbors=20, contamination=0.1, min_group_size=10):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.contamination = contamination\n",
    "        self.min_group_size = min_group_size\n",
    "        self.group_models = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        groups = X[X.columns[-1]]\n",
    "        features = X.iloc[:, :-1]\n",
    "        \n",
    "        for group in groups.unique():\n",
    "            group_mask = (groups == group)\n",
    "            group_features = features[group_mask]\n",
    "            \n",
    "            if len(group_features) >= self.min_group_size:\n",
    "                n_neighbors = min(self.n_neighbors, len(group_features) - 1)\n",
    "                lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=self.contamination, n_jobs=-1)\n",
    "                lof.fit(group_features)\n",
    "                self.group_models[group] = lof\n",
    "            else:\n",
    "                print(f\"Warning: Group {group} has fewer than {self.min_group_size} samples. Skipping LOF for this group.\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        groups = X[X.columns[-1]]\n",
    "        features = X.iloc[:, :-1]\n",
    "        outlier_scores = np.zeros(X.shape[0])\n",
    "        \n",
    "        for group in groups.unique():\n",
    "            group_mask = (groups == group)\n",
    "            group_features = features[group_mask]\n",
    "            \n",
    "            if group in self.group_models:\n",
    "                outlier_scores[group_mask] = -self.group_models[group].negative_outlier_factor_\n",
    "            else:\n",
    "                # For groups without a model (due to small size), assign a neutral score\n",
    "                outlier_scores[group_mask] = 0\n",
    "        \n",
    "        return outlier_scores.reshape(-1, 1)\n",
    "\n",
    "    def transform(self, X):\n",
    "        groups = X[X.columns[-1]]\n",
    "        features = X.iloc[:, :-1]\n",
    "        outlier_scores = np.zeros(X.shape[0])\n",
    "        \n",
    "        for group in groups.unique():\n",
    "            group_mask = (groups == group)\n",
    "            group_features = features[group_mask]\n",
    "            \n",
    "            if group in self.group_models:\n",
    "                outlier_scores[group_mask] = -self.group_models[group].negative_outlier_factor_\n",
    "            else:\n",
    "                # If we encounter a new group during transform, we'll use the global mean\n",
    "                outlier_scores[group_mask] = np.mean(-list(self.group_models.values())[0].negative_outlier_factor_)\n",
    "        \n",
    "        return outlier_scores.reshape(-1, 1)\n",
    "\n",
    "\n",
    "class TextOutlierTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, z_score_threshold=1.5):\n",
    "        self.z_score_threshold = z_score_threshold\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        combined_text = X['combined_text'].values\n",
    "        self.vectorizer.fit(combined_text)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        combined_text = X['combined_text'].values\n",
    "        description = X['description'].values\n",
    "        parent_asin = X['parent_asin'].values\n",
    "        \n",
    "        combined_text_tfidf = self.vectorizer.transform(combined_text)\n",
    "        description_tfidf = self.vectorizer.transform(description)\n",
    "        \n",
    "        outlier_scores = []\n",
    "        \n",
    "        for group_id in np.unique(parent_asin):\n",
    "            group_mask = (parent_asin == group_id)\n",
    "            group_texts = combined_text_tfidf[group_mask]\n",
    "            group_description = description_tfidf[group_mask][0]  # Assuming description is the same for all in group\n",
    "            \n",
    "            cosine_sim = cosine_similarity(group_texts, group_description)\n",
    "            \n",
    "            mean_sim = np.mean(cosine_sim)\n",
    "            std_sim = np.std(cosine_sim)\n",
    "            z_scores = (cosine_sim - mean_sim) / std_sim\n",
    "            \n",
    "            group_outlier_scores = (z_scores < -self.z_score_threshold).astype(float)\n",
    "            outlier_scores.extend(group_outlier_scores)\n",
    "        \n",
    "        return np.array(outlier_scores).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4abc1e7a-1fe2-48a7-a960-1eaf3a84bb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data preparation complete.\n",
      "Running outlier detection pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|                                                                                                | 0/5 [00:00<?, ?it/s]/tmp/ipykernel_23059/2645714462.py:187: RuntimeWarning: invalid value encountered in divide\n",
      "  z_scores = (cosine_sim - mean_sim) / std_sim\n",
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:14<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier detection complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing data...\")\n",
    "df['helpful_vote'] = pd.to_numeric(df['helpful_vote'], errors='coerce')\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "df['verified_purchase'] = df['verified_purchase'].astype(int)\n",
    "# Ensure text columns are converted to strings\n",
    "text_columns = ['title_review', 'text', 'main_category', 'title_meta', 'description']\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "df['combined_text'] = df['title_review'] + ' ' + df['text'] + ' ' + df['main_category'] + ' ' + df['title_meta'] + ' ' + df['description']\n",
    "# Handle missing values\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_columns:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "# For non-numeric columns, fill with a placeholder\n",
    "non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns\n",
    "for col in non_numeric_columns:\n",
    "    df[col] = df[col].fillna(\"Unknown\")\n",
    "# Convert timestamp to datetime and handle invalid values\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "feature_pipeline = ColumnTransformer([\n",
    "    ('num', StandardScaler(), ['rating', 'helpful_vote', 'verified_purchase', 'price', 'average_rating', 'rating_number']),\n",
    "    ('text', TextOutlierTransformer(z_score_threshold=1.5), ['combined_text', 'description', 'parent_asin']),\n",
    "    ('temporal', TemporalOutlierDetector(group_column='parent_asin'), ['timestamp', 'parent_asin']),\n",
    "    ('behavioral', BehavioralOutlierDetector(group_column='parent_asin'), ['user_id', 'timestamp', 'rating', 'parent_asin']),\n",
    "])\n",
    "\n",
    "print(\"Running outlier detection pipeline...\")\n",
    "with tqdm(total=5, desc=\"Processing\") as pbar:\n",
    "    X_transformed = feature_pipeline.fit_transform(df)\n",
    "    pbar.update(1)\n",
    "\n",
    "    X_transformed = np.asarray(X_transformed)\n",
    "    if X_transformed.ndim == 1:\n",
    "        X_transformed = X_transformed.reshape(-1, 1)\n",
    "\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42, n_jobs=-1)\n",
    "    iso_forest_outliers = iso_forest.fit_predict(X_transformed)\n",
    "    pbar.update(1)\n",
    "\n",
    "    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, n_jobs=-1)\n",
    "    lof_outliers = lof.fit_predict(X_transformed)\n",
    "    pbar.update(1)\n",
    "\n",
    "    df['isolation_forest_outlier'] = (iso_forest_outliers == -1).astype(int)\n",
    "    df['lof_outlier'] = (lof_outliers == -1).astype(int)\n",
    "    df['text_outlier'] = (X_transformed[:, 0] > 0.5).astype(int)\n",
    "    df['temporal_outlier'] = (X_transformed[:, 1] > 0.5).astype(int)\n",
    "    df['high_frequency_outlier'] = (X_transformed[:, 2] > 0.5).astype(int)\n",
    "    df['rating_deviation_outlier'] = (X_transformed[:, 3] > 0.5).astype(int)\n",
    "    pbar.update(1)\n",
    "\n",
    "    df['outlier_score'] = (df['isolation_forest_outlier'] + \n",
    "                           df['lof_outlier'] + \n",
    "                           df['text_outlier'] + \n",
    "                           df['temporal_outlier'] + \n",
    "                           df['high_frequency_outlier'] + \n",
    "                           df['rating_deviation_outlier'])\n",
    "    df['is_outlier'] = (df['outlier_score'] >= 2).astype(int)\n",
    "    pbar.update(1)\n",
    "\n",
    "print(\"Outlier detection complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbcea72c-f62b-47f9-a590-7df82b64d5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlier Detection Results:\n",
      "temporal_outlier: 550 outliers (5.50%)\n",
      "behavioral_outlier: 550 outliers (5.50%)\n",
      "isolation_forest_outlier: 990 outliers (9.90%)\n",
      "lof_outlier: 1000 outliers (10.00%)\n",
      "text_outlier: 6080 outliers (60.80%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "total_samples = len(df)\n",
    "print(f\"\\nOutlier Detection Results:\")\n",
    "for outlier_type in ['temporal_outlier', 'behavioral_outlier', 'isolation_forest_outlier', 'lof_outlier', 'text_outlier']:\n",
    "    count = df[outlier_type].sum()\n",
    "    print(f\"{outlier_type}: {count} outliers ({count/total_samples:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2391d7d-dde7-4870-8eb7-9c0beb8ced11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
